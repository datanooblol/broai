{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae67163-f7d3-4682-94b0-cd6fa366844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.12 (main, Apr  9 2025, 04:04:00) [Clang 20.1.0 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) # broai supports python3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11cdd5-a158-4627-b88c-4e51466dc87c",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2359ff-1d69-4d0b-b89b-3398913550da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc33f05-220d-427d-bc1b-fb86e791b0e5",
   "metadata": {},
   "source": [
    "# Setup for the Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2905a4-5c6a-4a2e-a3f3-29035ba7b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.prompt_management.core import Persona, Instructions, Example, Examples\n",
    "from broai.llm_management.ollama import BedrockOllamaChat\n",
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.agent_management.core import BroAgent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3631d3a9-af15-4a0d-b3d7-5fefb1580517",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_model = BedrockOllamaChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd1ad2c-c26f-403c-8f35-0a6d6bcd8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup:str = Field(description=\"this is a setup for the joke\")\n",
    "    punchline:str = Field(description=\"this is a punchline of the joke\")\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes:List[Joke]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f4841f-40fc-4d23-9e10-6300ad5984c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFormat(BaseModel):\n",
    "    message:str = Field(description=\"The user message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23871784-bd80-4345-adf5-0c2ac14cadf1",
   "metadata": {},
   "source": [
    "# BroAgent with full Framework: Happy Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7d2b14-1e5e-4fc0-a672-e1cf2aeaf6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes=[Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything'), Joke(setup=\"Why don't eggs tell jokes?\", punchline=\"They'd crack each other up\"), Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing'), Joke(setup='What do you call a fake noodle?', punchline='An impasta'), Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field'), Joke(setup=\"Why don't lobsters share?\", punchline=\"Because they're shellfish\"), Joke(setup=\"What do you call a can opener that doesn't work?\", punchline=\"A can't opener\"), Joke(setup='I told my wife she was drawing her eyebrows too high.', punchline='She looked surprised'), Joke(setup=\"Why don't some couples go to the gym?\", punchline=\"Because some relationships don't work out\"), Joke(setup='Why did the bicycle fall over?', punchline='Because it was two-tired')]\n",
      "CPU times: user 124 ms, sys: 13.8 ms, total: 138 ms\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Jokes,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=Jokes(jokes=[Joke(setup=\"error\", punchline=\"error\")])\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=InputFormat(message=\"Tell me ten jokes.\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16656b9f-66e7-4e03-8ea9-bcb912c33098",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Pydantic Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4946d76-e3ce-486d-8523-ba50a2053242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoy(BaseModel):\n",
    "    a:str\n",
    "    b:str\n",
    "    c:str\n",
    "    e:int\n",
    "    f:float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f35c23b1-07bf-4fc3-961c-20b048bacf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes=[Joke(setup='error', punchline='error')]\n",
      "CPU times: user 121 ms, sys: 0 ns, total: 121 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=Jokes(jokes=[Joke(setup=\"error\", punchline=\"error\")])\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94054f75-348b-4e97-9f4a-bc47ee765ab2",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Default Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7d5d9c-3b6a-4212-8ff0-3e08defa64b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown error\n",
      "CPU times: user 122 ms, sys: 0 ns, total: 122 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6d624-3232-4c13-95a2-ceaac703e623",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Custom Fallback in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78e0a40-5fc7-4c7a-b45e-e1b0d5b7c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a custom fallback in string\n",
      "CPU times: user 120 ms, sys: 162 Î¼s, total: 120 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=\"This is a custom fallback in string\"\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b55ec5-56b0-44e3-a0e1-d7a2ff2d4bae",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Custom Fallback not string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21d346f5-579e-4094-999d-0bc6aa986230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'This is another custom fallback'}\n",
      "CPU times: user 120 ms, sys: 0 ns, total: 120 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback={\"error\": \"This is another custom fallback\"}\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff499571-6302-45e4-8399-8c0b638d9831",
   "metadata": {},
   "source": [
    "# BroAgent with String: Happy Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b00a824-8378-4ccd-8b6c-ad4462bb8ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I gotcha! Here are ten jokes for ya:\n",
      "\n",
      "1. Setup: I told my wife she was drawing her eyebrows too high.\n",
      "Punchline: She looked surprised.\n",
      "\n",
      "2. Setup: Why don't scientists trust atoms?\n",
      "Punchline: Because they make up everything.\n",
      "\n",
      "3. Setup: Why don't eggs tell jokes?\n",
      "Punchline: They'd crack each other up.\n",
      "\n",
      "4. Setup: What do you call a fake noodle?\n",
      "Punchline: An impasta.\n",
      "\n",
      "5. Setup: Why did the scarecrow win an award?\n",
      "Punchline: Because he was outstanding in his field.\n",
      "\n",
      "6. Setup: What do you call a can opener that doesn't work?\n",
      "Punchline: A can't opener.\n",
      "\n",
      "7. Setup: I'm reading a book about anti-gravity.\n",
      "Punchline: It's impossible to put down.\n",
      "\n",
      "8. Setup: Why did the bicycle fall over?\n",
      "Punchline: Because it was two-tired.\n",
      "\n",
      "9. Setup: What do you call a bear with no socks on?\n",
      "Punchline: Barefoot.\n",
      "\n",
      "10. Setup: Why did the banana go to the doctor?\n",
      "Punchline: Because he wasn't peeling well.\n",
      "CPU times: user 39.1 ms, sys: 0 ns, total: 39.1 ms\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=\"tell some jokes based on message\",\n",
    "    structured_output=\"SETUP: \\nthe setup of the joke to build curiosity\\n\\nPUNCHLINE: \\nthe punchline is to complete the joke\"\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e2dc4c-4904-420f-814c-0ef6b98db99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_in_string = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45fb8e4-fcbe-4d94-b1ca-565f3e3aed60",
   "metadata": {},
   "source": [
    "# Bonus: Extract from normal string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f87002b-cfde-4fb8-b7e7-ed75027864e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# class InputFormat(BaseModel):\n",
    "#     content:str\n",
    "\n",
    "# pg = PromptGenerator(\n",
    "#     persona=\"You are a content extractor.\",\n",
    "#     instructions=Instructions(\n",
    "#         instructions=[\n",
    "#             \"Extract the content into the sepcified JSON formant.\",\n",
    "#         ],\n",
    "#     ),\n",
    "#     structured_output=Jokes,\n",
    "#     # examples=Examples(examples=[\n",
    "#     #     Example(\n",
    "#     #         setting=\"Joke Extraction\",\n",
    "#     #         input=InputFormat(content=joke_in_string),\n",
    "#     #         output=Jokes(jokes=[\n",
    "#     #             Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "#     #         ]),\n",
    "#     #     )\n",
    "#     # ]),\n",
    "#     fallback=Jokes(jokes=[Joke(setup=\"error\", punchline=\"error\")])\n",
    "# )\n",
    "\n",
    "# bro = BroAgent(\n",
    "#     prompt_generator=pg,\n",
    "#     model=bedrock_model\n",
    "# )\n",
    "\n",
    "# response = bro.run(request=InputFormat(content=joke_in_string))\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817ad0b-8de2-4009-aaf1-1731ec9123c0",
   "metadata": {},
   "source": [
    "# DuckStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e55064b3-3cb2-4618-a491-a35b087640b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.duckdb_management.utils import get_create_table_query, get_insert_query, get_batch_update_query\n",
    "from broai.duckdb_management.interface import DuckStoreInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4cd41f6-e407-4f34-99f6-4df98f5ac0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    \"doc_id\": \"VARCHAR\",\n",
    "    \"content\": \"VARCHAR\",\n",
    "    \"data\": \"JSON\"\n",
    "}\n",
    "\n",
    "sm = DuckStoreInterface(db=\"./duckmemory.db\", table=\"sessionmemory\", schemas=schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cadffe1a-f6d0-475f-8b1b-97828f52a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ffdda7-bbec-48e1-9b2d-c39dc3ee84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_id, content, data]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.sql_df(query=\"SELECT * FROM sessionmemory;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70327a72-0725-4e60-b214-d9b26e20a2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': 'VARCHAR', 'content': 'VARCHAR', 'data': 'JSON'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.show_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e70fdb-e7b1-42e9-bc58-a7baa9c6a293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id content  data\n",
       "0      0       a  None\n",
       "1      1       b  None"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data = [\n",
    "    [\"0\", \"a\"],\n",
    "    [\"1\", \"b\"]\n",
    "]\n",
    "data = \", \".join([f\"('{d[0]}', '{d[1]}')\" for d in _data])\n",
    "sm.add(fields=[\"doc_id\", \"content\"], data=data)\n",
    "sm.read(fields=[\"*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "293d5097-0fad-4a4b-8734-848d6b7f299b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id content  data\n",
       "0      0      aa  None\n",
       "1      1      bb  None"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data = [\n",
    "    [\"0\", \"aa\"],\n",
    "    [\"1\", \"bb\"]\n",
    "]\n",
    "data = \", \".join([f\"('{d[0]}', '{d[1]}')\" for d in _data])\n",
    "sm.update(schemas={\"doc_id\": \"VARCHAR\", \"content\": \"VARCHAR\"}, data=data, ref_keys=[\"doc_id\"])\n",
    "sm.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d684486-d736-4317-a11e-d8f2e48396d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id content  data\n",
       "0      0      aa  None"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete(where_condition=\"WHERE doc_id IN ('1', '2')\")\n",
    "sm.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7283aa8d-1bf6-4eeb-b106-5e4b1b3cf091",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f3edd9-fc8e-4ebe-8eba-5bc94fa254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.drop_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f975085b-4120-4526-aba8-070b7a9d78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.remove_database(confirm=\"remove database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7356245-ab9e-414a-aba1-1627b17d6536",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "756a3d3b-5503-4eef-9fba-082574ca5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.interface import Context, Contexts, TaskStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e58ae634-bbaa-42f5-ad32-196def4497bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context(id='3a419487-ffc3-47c3-98a7-4e6cdf13417a', context='Test', metadata=None, type='document', created_at='2025-04-21 23:41:52.356769')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Context(context=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a902d128-9c3e-480e-a186-110c0db162ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = Contexts(contexts=[Context(context=c, metadata={\"source\": s}) for c, s in zip([\"test1\", \"test2\"], [\"source1\", \"source2\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6462a3fb-1d42-4a0b-ad36-ca49640514e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge: \n",
      "\n",
      "Source: source1\n",
      "Context: \n",
      "test1\n",
      "\n",
      "Source: source2\n",
      "Context: \n",
      "test2\n"
     ]
    }
   ],
   "source": [
    "print(contexts.as_knowledge())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda9e248-121f-4d65-b570-510325cf7b0b",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75032c1-5306-4253-842f-a4fd39829887",
   "metadata": {},
   "source": [
    "## pdf_to_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a34672a7-4c5c-4627-bb22-4fa1c5e768af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_18850/2694963727.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: pdf_to_markdown\n",
      "  markdown_text, images = pdf_to_markdown(\"./docs/test1/storm.pdf\")\n",
      "Downloading layout model...: 100%|ââââââââââ| 5/5 [00:00<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading texify model...: 100%|ââââââââââ| 9/9 [00:01<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded texify model s3://texify/2025_02_18 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading text_recognition model...: 100%|ââââââââââ| 9/9 [00:28<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading table_recognition model...: 100%|ââââââââââ| 5/5 [00:00<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading text_detection model...: 100%|ââââââââââ| 6/6 [00:00<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading inline_math_detection model...: 100%|ââââââââââ| 5/5 [00:00<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ocr_error_detection model...: 100%|ââââââââââ| 8/8 [00:01<00:00,  6.41it/s]\n",
      "Recognizing layout: 100%|ââââââââââ| 5/5 [00:04<00:00,  1.00it/s]\n",
      "Running OCR Error Detection: 100%|ââââââââââ| 7/7 [00:00<00:00, 53.59it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Texify inference: 100%|ââââââââââ| 1/1 [00:01<00:00,  1.49s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|ââââââââââ| 2/2 [00:01<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.pdf_to_markdown import pdf_to_markdown\n",
    "\n",
    "markdown_text, images = pdf_to_markdown(\"./docs/test1/storm.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cecae8f5-7f93-47d2-ae2c-c0a004da6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./docs/test1/storm.md\", \"w\") as f:\n",
    "    f.write(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae2991-b027-4bf2-9789-888ae940c6d3",
   "metadata": {},
   "source": [
    "## chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb92fa35-16c9-4d4c-9657-806e83d75061",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./docs/test1/storm.md\", \"r\") as f:\n",
    "    markdown_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b35e5a48-7de5-411d-8c49-2707562bc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.experiments.chunk import split_markdown, consolidate_markdown, get_markdown_sections, split_overlap, chunk_chunks\n",
    "from broai.interface import Context, Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d92f3b57-ec86-49ad-b037-5455266bcedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown headings: max(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18850/3854812085.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_markdown\n",
      "  chunks = split_markdown(markdown_text)\n"
     ]
    }
   ],
   "source": [
    "chunks = split_markdown(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8e6aa14-e46b-4074-ac7e-42afe5d8548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef007eb4-3903-4e9c-8b62-88e280b0e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18850/169284658.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: consolidate_markdown\n",
      "  consolidated_chunks = consolidate_markdown(chunks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_chunks = consolidate_markdown(chunks)\n",
    "len(consolidated_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4da9c06-c3ef-4b78-a76e-00f8659b60b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18850/2669657907.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: get_markdown_sections\n",
      "  sections = get_markdown_sections(consolidated_chunks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = get_markdown_sections(consolidated_chunks)\n",
    "len(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b840acec-1a27-4d39-8031-b30e5f24b830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = Contexts()\n",
    "source = \".docs/test1/storm.md\"\n",
    "for section, chunk in zip(sections, consolidated_chunks):\n",
    "    contexts.add_context(Context(context=chunk, metadata={\"section\": section, \"source\": source, \"type\": \"document\"}))\n",
    "len(contexts.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f86b25e-3be5-4f0f-800b-9661b8053e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18850/1421024119.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_overlap\n",
      "  new_contexts = split_overlap(contexts.contexts)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_contexts = split_overlap(contexts.contexts)\n",
    "new_contexts = Contexts(contexts=new_contexts)\n",
    "len(new_contexts.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ade5498-fe67-4ddd-87fd-9c73e155d493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(id='e41a46f0-c213-4624-bbe3-831e29c6546d', context='# arXiv:2402.14207v2 [cs.CL] 8 Apr 2024\\n\\nAssisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\\n\\nYijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam\\n\\nStanford University\\n\\n{shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\\n', metadata={'section': '# arXiv:2402.14207v2 [cs.CL] 8 Apr 2024', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 0}, type='document', created_at='2025-04-21 23:43:05.567994'),\n",
       " Context(id='8e114076-b758-4e1d-abfe-0ee0f001b4b4', context=\"Abstract\\n\\nWe study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the *pre-writing* stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.\\n\\nFor evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outlinedriven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.\\n\", metadata={'section': 'Abstract', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 1}, type='document', created_at='2025-04-21 23:43:05.568054'),\n",
       " Context(id='eeded645-c6f1-474f-a12c-36d74b70f67f', context='1 Introduction\\n\\nLarge language models (LLMs) have demonstrated impressive writing capabilities [\\\\(Yang et al.,](#page-12-0) [2023;](#page-12-0) [Pavlik,](#page-10-0) [2023;](#page-10-0) [Wenzlaff and Spaeth,](#page-11-0) [2022;](#page-11-0) [Fitria,](#page-9-0) [2023\\\\)](#page-9-0), but it is unclear how we can use them to write grounded, long-form articles, like full-length Wikipedia pages. Such expository writing, which seeks to inform the reader on a topic in an organized manner [\\\\(Weaver III and Kintsch,](#page-11-1) [1991;](#page-11-1) [Balepur et al.,](#page-9-1) [2023\\\\)](#page-9-1), requires thorough research and planning in the *pre-writing* stage [\\\\(Rohman,](#page-11-2)\\n\\n<span id=\"page-0-0\"></span>![](_page_0_Picture_10.jpeg)\\n\\nFigure 1: We explore writing Wikipedia-like articles from scratch, which demands a pre-writing stage before producing the article. In this stage, simpler approaches like Direct Prompting have limited planning capacity. In contrast, STORM researches the topic via perspectiveguided question asking in simulated conversations.\\n\\n[1965\\\\)](#page-11-2), even before the actual writing process can start. However, prior work on generating Wikipedia articles [\\\\(Banerjee and Mitra,](#page-9-2) [2015;](#page-9-2) [MinguillÃ³n](#page-10-1) [et al.,](#page-10-1) [2017;](#page-10-1) [Liu et al.,](#page-10-2) [2018;](#page-10-2) [Fan and Gardent,](#page-9-3) [2022\\\\)](#page-9-3) has generally bypassed the pre-writing stage: for instance, [Liu et al.](#page-10-2) [\\\\(2018\\\\)](#page-10-2) presume reference documents are provided in advance, while [Fan and](#page-9-3) [Gardent](#page-9-3) [\\\\(2022\\\\)](#page-9-3) assume an article outline is available and focus on expanding each section. These assumptions do not hold in general, as collecting references and crafting outlines demand advanced information literacy skills [\\\\(Doyle,](#page-9-4) [1994\\\\)](#page-9-4) to identify, evaluate, and organize external sources - a task that is challenging even for experienced writers. Automating this process can facilitate individuals in initiating in-depth learning about a topic and greatly reduce the expensive expert hours necessary for their expository writing.\\n\\nWe explore these challenges by focusing on how to generate Wikipedia-like articles *from scratch*. We decompose this problem into two tasks. The first is to conduct research to generate an outline, *i.e.*, a list of multi-level sections, and collect a set of reference documents. The second uses the outline and the references to produce the full-length article. Such a task decomposition mirrors the human writing process which usually includes phases of pre-writing, drafting, and revising [\\\\(Rohman,](#page-11-2) [1965;](#page-11-2) [Munoz-Luna,](#page-10-3) [2015\\\\)](#page-10-3).\\n\\nAs pre-trained language models inherently possess a wealth of knowledge, a direct approach is to rely on their parametric knowledge for generating outlines or even entire articles (*Direct Gen*). However, this approach is limited by a lack of details and hallucinations [\\\\(Xu et al.,](#page-11-3) [2023\\\\)](#page-11-3), particularly in addressing long-tail topics [\\\\(Kandpal et al.,](#page-10-4) [2023\\\\)](#page-10-4). This underscores the importance of leveraging external sources, and current strategies often involve retrieval-augmented generation (*RAG*), which circles back to the problem of researching the topic in the pre-writing stage, as much information cannot be surfaced through simple topic searches.\\n\\nHuman learning theories [\\\\(Tawfik et al.,](#page-11-4) [2020;](#page-11-4) [Booth et al.,](#page-9-5) [2003\\\\)](#page-9-5) highlight *asking effective questions* in information acquisition. Although instruction-tuned models [\\\\(Ouyang et al.,](#page-10-5) [2022\\\\)](#page-10-5) can be prompted directly to generate questions, we find that they typically produce basic \"What\", \"When\", and \"Where\" questions (Figure [1](#page-0-0) (A)) which often only address surface-level facts about the topic. To endow LLMs with the capacity to conduct better research, we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking.\\n\\nThe', metadata={'section': '1 Introduction', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 2}, type='document', created_at='2025-04-21 23:43:05.601910')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_contexts.contexts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f9e613d-dc5b-49a0-86e5-683d73d0e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] | tokens: 35 | chars: 309\n",
      "[1] | tokens: 189 | chars: 1349\n",
      "[2] | tokens: 500 | chars: 4170\n",
      "[3] | tokens: 462 | chars: 3550\n",
      "[4] | tokens: 500 | chars: 2030\n",
      "[5] | tokens: 211 | chars: 1257\n",
      "[6] | tokens: 162 | chars: 1096\n",
      "[7] | tokens: 238 | chars: 1830\n",
      "[8] | tokens: 196 | chars: 2327\n",
      "[9] | tokens: 256 | chars: 1831\n",
      "[10] | tokens: 226 | chars: 1627\n",
      "[11] | tokens: 114 | chars: 759\n",
      "[12] | tokens: 150 | chars: 1002\n",
      "[13] | tokens: 57 | chars: 422\n",
      "[14] | tokens: 166 | chars: 1305\n",
      "[15] | tokens: 105 | chars: 681\n",
      "[16] | tokens: 500 | chars: 1605\n",
      "[17] | tokens: 500 | chars: 1131\n",
      "[18] | tokens: 500 | chars: 868\n",
      "[19] | tokens: 480 | chars: 1431\n",
      "[20] | tokens: 500 | chars: 3018\n",
      "[21] | tokens: 320 | chars: 924\n",
      "[22] | tokens: 196 | chars: 1381\n",
      "[23] | tokens: 500 | chars: 2242\n",
      "[24] | tokens: 500 | chars: 3699\n",
      "[25] | tokens: 260 | chars: 1890\n",
      "[26] | tokens: 441 | chars: 3970\n",
      "[27] | tokens: 88 | chars: 637\n",
      "[28] | tokens: 159 | chars: 1194\n",
      "[29] | tokens: 62 | chars: 413\n",
      "[30] | tokens: 230 | chars: 1487\n",
      "[31] | tokens: 500 | chars: 4696\n",
      "[32] | tokens: 500 | chars: 4683\n",
      "[33] | tokens: 500 | chars: 4837\n",
      "[34] | tokens: 500 | chars: 5164\n",
      "[35] | tokens: 500 | chars: 5514\n",
      "[36] | tokens: 500 | chars: 5094\n",
      "[37] | tokens: 281 | chars: 2320\n",
      "[38] | tokens: 191 | chars: 1398\n",
      "[39] | tokens: 87 | chars: 724\n",
      "[40] | tokens: 500 | chars: 3178\n",
      "[41] | tokens: 500 | chars: 2304\n",
      "[42] | tokens: 500 | chars: 2312\n",
      "[43] | tokens: 299 | chars: 1590\n",
      "[44] | tokens: 124 | chars: 883\n",
      "[45] | tokens: 20 | chars: 232\n",
      "[46] | tokens: 500 | chars: 2032\n",
      "[47] | tokens: 500 | chars: 871\n",
      "[48] | tokens: 500 | chars: 986\n",
      "[49] | tokens: 500 | chars: 755\n",
      "[50] | tokens: 500 | chars: 864\n",
      "[51] | tokens: 500 | chars: 882\n",
      "[52] | tokens: 500 | chars: 1274\n",
      "[53] | tokens: 500 | chars: 1152\n",
      "[54] | tokens: 500 | chars: 1276\n",
      "[55] | tokens: 500 | chars: 2068\n",
      "[56] | tokens: 314 | chars: 1752\n",
      "[57] | tokens: 250 | chars: 1737\n",
      "[58] | tokens: 181 | chars: 1720\n",
      "[59] | tokens: 75 | chars: 613\n",
      "[60] | tokens: 77 | chars: 594\n",
      "[61] | tokens: 99 | chars: 685\n",
      "[62] | tokens: 121 | chars: 896\n",
      "[63] | tokens: 500 | chars: 1743\n",
      "[64] | tokens: 500 | chars: 1425\n",
      "[65] | tokens: 500 | chars: 1261\n",
      "[66] | tokens: 500 | chars: 759\n",
      "[67] | tokens: 500 | chars: 942\n",
      "[68] | tokens: 500 | chars: 1206\n",
      "[69] | tokens: 500 | chars: 826\n",
      "[70] | tokens: 500 | chars: 606\n",
      "[71] | tokens: 412 | chars: 795\n",
      "[72] | tokens: 291 | chars: 1933\n",
      "[73] | tokens: 208 | chars: 1347\n",
      "[74] | tokens: 300 | chars: 1821\n",
      "[75] | tokens: 236 | chars: 1519\n",
      "[76] | tokens: 147 | chars: 952\n",
      "[77] | tokens: 197 | chars: 1297\n",
      "[78] | tokens: 48 | chars: 319\n",
      "[79] | tokens: 93 | chars: 601\n",
      "[80] | tokens: 83 | chars: 505\n",
      "[81] | tokens: 62 | chars: 382\n",
      "[82] | tokens: 90 | chars: 631\n",
      "[83] | tokens: 190 | chars: 1229\n",
      "[84] | tokens: 219 | chars: 1497\n"
     ]
    }
   ],
   "source": [
    "chunk_chunks([c.context for c in new_contexts.contexts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f7b86-1837-4a5b-aeda-bf24fbb9f2b5",
   "metadata": {},
   "source": [
    "## Enmedding: BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0d51143-5bcc-4414-b85a-c8e63d8d1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "155469bb-f92d-426d-9b95-8f1436ebdea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmbeddingDimension.BAAI_BGE_M3.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca8cd6b5-71ac-4d98-9ded-4747cae64ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18850/107190707.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|ââââââââââ| 30/30 [00:08<00:00,  3.55it/s]\n"
     ]
    }
   ],
   "source": [
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89fddf17-ba37-442c-b200-98a7a6522b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfd9f608-46dd-426e-b2cf-5df652d1531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03412  , -0.047    , -0.0009174, ...,  0.04828  ,  0.00756  ,\n",
       "        -0.0296   ],\n",
       "       [-0.010376 , -0.04483  , -0.02428  , ..., -0.00822  ,  0.01502  ,\n",
       "         0.011086 ]], shape=(2, 1024), dtype=float16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baai_em.run(sentences_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87da540-9127-4067-8b20-a6b35ba9d2aa",
   "metadata": {},
   "source": [
    "## CrossEncoder: cross-encoder/ms-marco-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e650ce5-d3d2-47c1-964d-9590126c5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "from broai.interface import Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7ba15a8-b467-4abd-b80c-c2ef011e883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18850/3697317191.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72f2ec27-fd77-4ad2-bb7e-5601e07063a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"pandas is good\"\n",
    "contexts = [Context(context=con, metadata={\"source\":\"test\"}) for con in [\"pandas is goose\", \"pandas is good\", \"pandas is great\", \"pandas is goat\", \"pandas is gang\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c314e0b1-9ca8-4de9-8278-91527eb2e37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(id='64225d55-6a8f-4150-93d9-6b8870ae8f17', context='pandas is good', metadata={'source': 'test'}, type='document', created_at='2025-04-21 23:43:26.513680'),\n",
       " Context(id='40358aa6-9779-4405-b937-af74e488c0f6', context='pandas is great', metadata={'source': 'test'}, type='document', created_at='2025-04-21 23:43:26.513698'),\n",
       " Context(id='5e2e901b-6dcf-4e20-a040-1349c53b5597', context='pandas is goat', metadata={'source': 'test'}, type='document', created_at='2025-04-21 23:43:26.513710')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_contexts, scores = rr.run(query, contexts, top_n=3)\n",
    "reranked_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40fd3f40-47c5-4d62-bf55-5762bfdbf1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.630863189697266, 7.362998962402344, 0.6360796689987183]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef4566-b0d1-41f4-a2f2-d621fd6869c5",
   "metadata": {},
   "source": [
    "## ExpBroAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f26fbda9-4aeb-40d8-8251-53d4d85aaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.experiments.bro_agent import BroAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f5a03fa-e8ec-4ec7-b148-766751b7fbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes=[Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything'), Joke(setup=\"Why don't eggs tell jokes?\", punchline=\"They'd crack each other up\"), Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing'), Joke(setup='What do you call a fake noodle?', punchline='An impasta'), Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field'), Joke(setup=\"Why don't lobsters share?\", punchline=\"Because they're shellfish\"), Joke(setup=\"What do you call a can opener that doesn't work?\", punchline=\"A can't opener\"), Joke(setup='I told my wife she was drawing her eyebrows too high.', punchline='She looked surprised'), Joke(setup=\"Why don't some couples go to the gym?\", punchline=\"Because some relationships don't work out\"), Joke(setup='Why did the bicycle fall over?', punchline='Because it was two-tired'), Joke(setup='What do you call a bear with no socks on?', punchline='Barefoot'), Joke(setup='Why did the banana go to the doctor?', punchline=\"Because he wasn't peeling well\"), Joke(setup='Why did the baker go to the bank?', punchline='He needed dough'), Joke(setup='Why did the mushroom go to the party?', punchline='Because he was a fun-gi'), Joke(setup='Why did the cat join a band?', punchline='Because he wanted to be the purr-cussionist'), Joke(setup='What do you call a group of cows playing instruments?', punchline='A moo-sical band'), Joke(setup='Why did the computer go to the doctor?', punchline='It had a virus'), Joke(setup='Why did the kid bring a ladder to school?', punchline='He wanted to reach his full potential'), Joke(setup='What do you call a dog that does magic tricks?', punchline='A labracadabrador'), Joke(setup='Why did the orange stop in the middle of the road?', punchline='Because it ran out of juice'), Joke(setup='Why did the chicken go to the doctor?', punchline='Because it had fowl breath')]\n",
      "CPU times: user 44.1 ms, sys: 31 Î¼s, total: 44.2 ms\n",
      "Wall time: 5.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=Persona(name=\"Bro Andy\", description=\"You are the best bro who's cool and supportive.\"),\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Jokes,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=Jokes(jokes=[Joke(setup=\"error\", punchline=\"error\")])\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=InputFormat(message=\"Tell me twenty jokes.\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b550e74-e20a-426c-878b-59b0e7bd6a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: 1\n",
      "setup: Why don't scientists trust atoms?\n",
      "punchline: Because they make up everything\n",
      "====================\n",
      "Joke: 2\n",
      "setup: Why don't eggs tell jokes?\n",
      "punchline: They'd crack each other up\n",
      "====================\n",
      "Joke: 3\n",
      "setup: Why did the tomato turn red?\n",
      "punchline: Because it saw the salad dressing\n",
      "====================\n",
      "Joke: 4\n",
      "setup: What do you call a fake noodle?\n",
      "punchline: An impasta\n",
      "====================\n",
      "Joke: 5\n",
      "setup: Why did the scarecrow win an award?\n",
      "punchline: Because he was outstanding in his field\n",
      "====================\n",
      "Joke: 6\n",
      "setup: Why don't lobsters share?\n",
      "punchline: Because they're shellfish\n",
      "====================\n",
      "Joke: 7\n",
      "setup: What do you call a can opener that doesn't work?\n",
      "punchline: A can't opener\n",
      "====================\n",
      "Joke: 8\n",
      "setup: I told my wife she was drawing her eyebrows too high.\n",
      "punchline: She looked surprised\n",
      "====================\n",
      "Joke: 9\n",
      "setup: Why don't some couples go to the gym?\n",
      "punchline: Because some relationships don't work out\n",
      "====================\n",
      "Joke: 10\n",
      "setup: Why did the bicycle fall over?\n",
      "punchline: Because it was two-tired\n",
      "====================\n",
      "Joke: 11\n",
      "setup: What do you call a bear with no socks on?\n",
      "punchline: Barefoot\n",
      "====================\n",
      "Joke: 12\n",
      "setup: Why did the banana go to the doctor?\n",
      "punchline: Because he wasn't peeling well\n",
      "====================\n",
      "Joke: 13\n",
      "setup: Why did the baker go to the bank?\n",
      "punchline: He needed dough\n",
      "====================\n",
      "Joke: 14\n",
      "setup: Why did the mushroom go to the party?\n",
      "punchline: Because he was a fun-gi\n",
      "====================\n",
      "Joke: 15\n",
      "setup: Why did the cat join a band?\n",
      "punchline: Because he wanted to be the purr-cussionist\n",
      "====================\n",
      "Joke: 16\n",
      "setup: What do you call a group of cows playing instruments?\n",
      "punchline: A moo-sical band\n",
      "====================\n",
      "Joke: 17\n",
      "setup: Why did the computer go to the doctor?\n",
      "punchline: It had a virus\n",
      "====================\n",
      "Joke: 18\n",
      "setup: Why did the kid bring a ladder to school?\n",
      "punchline: He wanted to reach his full potential\n",
      "====================\n",
      "Joke: 19\n",
      "setup: What do you call a dog that does magic tricks?\n",
      "punchline: A labracadabrador\n",
      "====================\n",
      "Joke: 20\n",
      "setup: Why did the orange stop in the middle of the road?\n",
      "punchline: Because it ran out of juice\n",
      "====================\n",
      "Joke: 21\n",
      "setup: Why did the chicken go to the doctor?\n",
      "punchline: Because it had fowl breath\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for enum, j in enumerate(response.jokes):\n",
    "    print(\"Joke:\", enum+1)\n",
    "    print(\"setup:\", j.setup)\n",
    "    print(\"punchline:\", j.punchline)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484803a-13f5-497f-8868-3636720bdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai",
   "language": "python",
   "name": "broai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
