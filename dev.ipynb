{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae67163-f7d3-4682-94b0-cd6fa366844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.12 (main, Apr  9 2025, 04:04:00) [Clang 20.1.0 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) # broai supports python3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11cdd5-a158-4627-b88c-4e51466dc87c",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2359ff-1d69-4d0b-b89b-3398913550da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc33f05-220d-427d-bc1b-fb86e791b0e5",
   "metadata": {},
   "source": [
    "# Setup for the Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2905a4-5c6a-4a2e-a3f3-29035ba7b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.prompt_management.core import Persona, Instructions, Example, Examples\n",
    "from broai.llm_management.ollama import BedrockOllamaChat\n",
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.agent_management.core import BroAgent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3631d3a9-af15-4a0d-b3d7-5fefb1580517",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_model = BedrockOllamaChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd1ad2c-c26f-403c-8f35-0a6d6bcd8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup:str = Field(description=\"this is a setup for the joke\")\n",
    "    punchline:str = Field(description=\"this is a punchline of the joke\")\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes:List[Joke]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f4841f-40fc-4d23-9e10-6300ad5984c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFormat(BaseModel):\n",
    "    message:str = Field(description=\"The user message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23871784-bd80-4345-adf5-0c2ac14cadf1",
   "metadata": {},
   "source": [
    "# BroAgent with full Framework: Happy Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7d2b14-1e5e-4fc0-a672-e1cf2aeaf6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes=[Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything'), Joke(setup=\"Why don't eggs tell jokes?\", punchline=\"They'd crack each other up\"), Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing'), Joke(setup='What do you call a fake noodle?', punchline='An impasta'), Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field'), Joke(setup=\"Why don't lobsters share?\", punchline=\"Because they're shellfish\"), Joke(setup=\"What do you call a can opener that doesn't work?\", punchline=\"A can't opener\"), Joke(setup='I told my wife she was drawing her eyebrows too high.', punchline='She looked surprised'), Joke(setup=\"Why don't some couples go to the gym?\", punchline=\"Because some relationships don't work out\"), Joke(setup='Why did the bicycle fall over?', punchline='Because it was two-tired')]\n",
      "CPU times: user 84.2 ms, sys: 15.8 ms, total: 100 ms\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Jokes,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=Jokes(jokes=[Joke(setup=\"error\", punchline=\"error\")])\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=InputFormat(message=\"Tell me ten jokes.\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16656b9f-66e7-4e03-8ea9-bcb912c33098",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Pydantic Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4946d76-e3ce-486d-8523-ba50a2053242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoy(BaseModel):\n",
    "    a:str\n",
    "    b:str\n",
    "    c:str\n",
    "    e:int\n",
    "    f:float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f35c23b1-07bf-4fc3-961c-20b048bacf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes=[Joke(setup='error', punchline='error')]\n",
      "CPU times: user 121 ms, sys: 0 ns, total: 121 ms\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=Jokes(jokes=[Joke(setup=\"error\", punchline=\"error\")])\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94054f75-348b-4e97-9f4a-bc47ee765ab2",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Default Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7d5d9c-3b6a-4212-8ff0-3e08defa64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown error\n",
      "CPU times: user 120 ms, sys: 588 Î¼s, total: 121 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6d624-3232-4c13-95a2-ceaac703e623",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Custom Fallback in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78e0a40-5fc7-4c7a-b45e-e1b0d5b7c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a custom fallback in string\n",
      "CPU times: user 121 ms, sys: 0 ns, total: 121 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=\"This is a custom fallback in string\"\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b55ec5-56b0-44e3-a0e1-d7a2ff2d4bae",
   "metadata": {},
   "source": [
    "# BroAgent Full Framework: Custom Fallback not string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21d346f5-579e-4094-999d-0bc6aa986230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'This is another custom fallback'}\n",
      "CPU times: user 120 ms, sys: 396 Î¼s, total: 120 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"tell some jokes based on message\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Decoy,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Funny Andy\",\n",
    "            input=InputFormat(message=\"Gimme three jokes\"),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback={\"error\": \"This is another custom fallback\"}\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff499571-6302-45e4-8399-8c0b638d9831",
   "metadata": {},
   "source": [
    "# BroAgent with String: Happy Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b00a824-8378-4ccd-8b6c-ad4462bb8ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I gotcha! Here are ten jokes for ya:\n",
      "\n",
      "1. Setup: I told my wife she was drawing her eyebrows too high.\n",
      "Punchline: She looked surprised.\n",
      "\n",
      "2. Setup: Why don't scientists trust atoms?\n",
      "Punchline: Because they make up everything.\n",
      "\n",
      "3. Setup: Why don't eggs tell jokes?\n",
      "Punchline: They'd crack each other up.\n",
      "\n",
      "4. Setup: What do you call a fake noodle?\n",
      "Punchline: An impasta.\n",
      "\n",
      "5. Setup: Why did the scarecrow win an award?\n",
      "Punchline: Because he was outstanding in his field.\n",
      "\n",
      "6. Setup: What do you call a can opener that doesn't work?\n",
      "Punchline: A can't opener.\n",
      "\n",
      "7. Setup: I'm reading a book about anti-gravity.\n",
      "Punchline: It's impossible to put down.\n",
      "\n",
      "8. Setup: Why did the bicycle fall over?\n",
      "Punchline: Because it was two-tired.\n",
      "\n",
      "9. Setup: What do you call a bear with no socks on?\n",
      "Punchline: Barefoot.\n",
      "\n",
      "10. Setup: Why did the banana go to the doctor?\n",
      "Punchline: Because he wasn't peeling well.\n",
      "CPU times: user 39.4 ms, sys: 164 Î¼s, total: 39.6 ms\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are the good bro Andy.\",\n",
    "    instructions=\"tell some jokes based on message\",\n",
    "    structured_output=\"SETUP: \\nthe setup of the joke to build curiosity\\n\\nPUNCHLINE: \\nthe punchline is to complete the joke\"\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=\"Tell me ten jokes.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2e2dc4c-4904-420f-814c-0ef6b98db99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_in_string = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45fb8e4-fcbe-4d94-b1ca-565f3e3aed60",
   "metadata": {},
   "source": [
    "# Bonus: Extract from normal string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f87002b-cfde-4fb8-b7e7-ed75027864e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes=[Joke(setup='I told my wife she was drawing her eyebrows too high.', punchline='She looked surprised.'), Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything.'), Joke(setup=\"Why don't eggs tell jokes?\", punchline=\"They'd crack each other up.\"), Joke(setup='What do you call a fake noodle?', punchline='An impasta.'), Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field.'), Joke(setup=\"What do you call a can opener that doesn't work?\", punchline=\"A can't opener.\"), Joke(setup=\"I'm reading a book about anti-gravity.\", punchline=\"It's impossible to put down.\"), Joke(setup='Why did the bicycle fall over?', punchline='Because it was two-tired.'), Joke(setup='What do you call a bear with no socks on?', punchline='Barefoot.'), Joke(setup='Why did the banana go to the doctor?', punchline=\"Because he wasn't peeling well.\")]\n",
      "CPU times: user 40.8 ms, sys: 2.83 ms, total: 43.7 ms\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class InputFormat(BaseModel):\n",
    "    content:str\n",
    "\n",
    "pg = PromptGenerator(\n",
    "    persona=\"You are a content extractor.\",\n",
    "    instructions=Instructions(\n",
    "        instructions=[\n",
    "            \"Extract the content into the sepcified JSON formant.\",\n",
    "        ],\n",
    "    ),\n",
    "    structured_output=Jokes,\n",
    "    examples=Examples(examples=[\n",
    "        Example(\n",
    "            setting=\"Joke Extraction\",\n",
    "            input=InputFormat(content=joke_in_string),\n",
    "            output=Jokes(jokes=[\n",
    "                Joke(setup=\"the setup of the joke to build curiosity\", punchline=\"the punchline is to complete the joke\")\n",
    "            ]),\n",
    "        )\n",
    "    ]),\n",
    "    fallback=Jokes(jokes=[Joke(setup=\"error\", punchline=\"error\")])\n",
    ")\n",
    "\n",
    "bro = BroAgent(\n",
    "    prompt_generator=pg,\n",
    "    model=bedrock_model\n",
    ")\n",
    "\n",
    "response = bro.run(request=InputFormat(content=joke_in_string))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817ad0b-8de2-4009-aaf1-1731ec9123c0",
   "metadata": {},
   "source": [
    "# DuckStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55064b3-3cb2-4618-a491-a35b087640b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.duckdb_management.utils import get_create_table_query, get_insert_query, get_batch_update_query\n",
    "from broai.duckdb_management.interface import DuckStoreInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4cd41f6-e407-4f34-99f6-4df98f5ac0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    \"doc_id\": \"VARCHAR\",\n",
    "    \"content\": \"VARCHAR\",\n",
    "    \"data\": \"JSON\"\n",
    "}\n",
    "\n",
    "sm = DuckStoreInterface(db=\"./duckmemory.db\", table=\"sessionmemory\", schemas=schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadffe1a-f6d0-475f-8b1b-97828f52a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ffdda7-bbec-48e1-9b2d-c39dc3ee84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_id, content, data]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.sql_df(query=\"SELECT * FROM sessionmemory;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70327a72-0725-4e60-b214-d9b26e20a2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': 'VARCHAR', 'content': 'VARCHAR', 'data': 'JSON'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.show_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58e70fdb-e7b1-42e9-bc58-a7baa9c6a293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id content  data\n",
       "0      0       a  None\n",
       "1      1       b  None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data = [\n",
    "    [\"0\", \"a\"],\n",
    "    [\"1\", \"b\"]\n",
    "]\n",
    "data = \", \".join([f\"('{d[0]}', '{d[1]}')\" for d in _data])\n",
    "sm.add(fields=[\"doc_id\", \"content\"], data=data)\n",
    "sm.read(fields=[\"*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293d5097-0fad-4a4b-8734-848d6b7f299b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id content  data\n",
       "0      0      aa  None\n",
       "1      1      bb  None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data = [\n",
    "    [\"0\", \"aa\"],\n",
    "    [\"1\", \"bb\"]\n",
    "]\n",
    "data = \", \".join([f\"('{d[0]}', '{d[1]}')\" for d in _data])\n",
    "sm.update(schemas={\"doc_id\": \"VARCHAR\", \"content\": \"VARCHAR\"}, data=data, ref_keys=[\"doc_id\"])\n",
    "sm.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d684486-d736-4317-a11e-d8f2e48396d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>content</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id content  data\n",
       "0      0      aa  None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete(where_condition=\"WHERE doc_id IN ('1', '2')\")\n",
    "sm.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7283aa8d-1bf6-4eeb-b106-5e4b1b3cf091",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97f3edd9-fc8e-4ebe-8eba-5bc94fa254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.drop_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f975085b-4120-4526-aba8-070b7a9d78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.remove_database(confirm=\"remove database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7356245-ab9e-414a-aba1-1627b17d6536",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "756a3d3b-5503-4eef-9fba-082574ca5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.interface import Context, TaskStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58ae634-bbaa-42f5-ad32-196def4497bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context(id='472d878e-2b0b-4f43-9876-3045455b0855', context='Test', metadata=None, type='document', created_at='2025-04-20 16:26:08.193622')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Context(context=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3063400-e937-42f7-8fb8-23dd12442147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TaskStatus.NOT_STARTED: 'not_started'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskStatus.NOT_STARTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f37a5cb-7960-4c84-be08-1e1fcc940128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TaskStatus.PENDING: 'pending'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskStatus.PENDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf0c00fb-ab0c-4c94-bff5-ae770287ce90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TaskStatus.DONE: 'done'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskStatus.DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda9e248-121f-4d65-b570-510325cf7b0b",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75032c1-5306-4253-842f-a4fd39829887",
   "metadata": {},
   "source": [
    "## pdf_to_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34672a7-4c5c-4627-bb22-4fa1c5e768af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24612/2694963727.py:3: UserWarning: [EXPERIMENT] You're using an experimental function: pdf_to_markdown\n",
      "  markdown_text, images = pdf_to_markdown(\"./docs/test1/storm.pdf\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded texify model s3://texify/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|ââââââââââ| 5/5 [00:03<00:00,  1.33it/s]\n",
      "Running OCR Error Detection: 100%|ââââââââââ| 7/7 [00:00<00:00, 65.98it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Texify inference: 100%|ââââââââââ| 1/1 [00:01<00:00,  1.44s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|ââââââââââ| 2/2 [00:01<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.pdf_to_markdown import pdf_to_markdown\n",
    "\n",
    "markdown_text, images = pdf_to_markdown(\"./docs/test1/storm.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecae8f5-7f93-47d2-ae2c-c0a004da6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./docs/test1/storm.md\", \"w\") as f:\n",
    "    f.write(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f7b86-1837-4a5b-aeda-bf24fbb9f2b5",
   "metadata": {},
   "source": [
    "## Enmedding: BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5346d870-18b7-4bf4-9264-ea312cded0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|ââââââââââ| 30/30 [00:11<00:00,  2.51it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.626  0.3477]\n",
      " [0.3499 0.678 ]]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "sentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\n",
    "sentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n",
    "               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)\n",
    "# [[0.6265, 0.3477], [0.3499, 0.678 ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b4a6b7-1146-402a-aa69-8270349d1be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 ms, sys: 0 ns, total: 39.2 ms\n",
      "Wall time: 38.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_1 = model.encode(sentences_1, \n",
    "                            batch_size=12, \n",
    "                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "                            )['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6deed8f6-5202-4201-81b5-db75bcc5fefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.3 ms, sys: 334 Î¼s, total: 38.6 ms\n",
      "Wall time: 38.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_2 = model.encode(sentences_2)['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b947555b-7a38-4d1e-a65f-388285e6124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.5 ms, sys: 0 ns, total: 39.5 ms\n",
      "Wall time: 39.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_3 = model.encode([\n",
    "    sentences_1,\n",
    "    sentences_2,\n",
    "    sentences_1,\n",
    "    sentences_2,\n",
    "    sentences_1,\n",
    "    sentences_2,\n",
    "    sentences_1,\n",
    "    sentences_2,\n",
    "])['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e6e504-7c3d-4067-843a-efb677ee332d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02412 , -0.05185 , -0.01099 , ...,  0.0371  ,  0.02528 ,\n",
       "        -0.01452 ],\n",
       "       [ 0.01265 , -0.0691  ,  0.002552, ...,  0.02354 ,  0.001125,\n",
       "         0.00848 ],\n",
       "       [-0.02412 , -0.05185 , -0.01099 , ...,  0.0371  ,  0.02528 ,\n",
       "        -0.01452 ],\n",
       "       ...,\n",
       "       [ 0.01265 , -0.0691  ,  0.002552, ...,  0.02354 ,  0.001125,\n",
       "         0.00848 ],\n",
       "       [-0.02412 , -0.05185 , -0.01099 , ...,  0.0371  ,  0.02528 ,\n",
       "        -0.01452 ],\n",
       "       [ 0.01265 , -0.0691  ,  0.002552, ...,  0.02354 ,  0.001125,\n",
       "         0.00848 ]], shape=(8, 1024), dtype=float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87da540-9127-4067-8b20-a6b35ba9d2aa",
   "metadata": {},
   "source": [
    "## CrossEncoder: cross-encoder/ms-marco-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070f2cc2-292a-492a-b5da-0f97bd0ad23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.75 s, sys: 692 ms, total: 6.45 s\n",
      "Wall time: 5.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4ed528-712f-4c6e-9800-bf9fd9f0e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"pandas is goose\", \"pandas is good\", \"pandas is great\", \"pandas is goat\", \"pandas is gang\"]\n",
    "scores = model.predict([[\"pandas is good\", q] for q in query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3d915-dfd0-4d89-8c0b-6a60cb1c1a47",
   "metadata": {},
   "source": [
    "## rerank_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865e2115-3acb-42f3-8f68-83fb1995c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.experiments.rerank import rerank_contexts\n",
    "from broai.interface import Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "718485a8-64a3-4dfd-8400-d7a41e676ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24612/4053318438.py:1: UserWarning: [EXPERIMENT] You're using an experimental function: rerank_contexts\n",
      "  reranked_contexts = rerank_contexts(query, scores, top_n=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['pandas is good', 'pandas is great', 'pandas is goat'],\n",
       " [8.630863189697266, 7.362998962402344, 0.6360796689987183])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_contexts = rerank_contexts(query, scores, top_n=3)\n",
    "reranked_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae2991-b027-4bf2-9789-888ae940c6d3",
   "metadata": {},
   "source": [
    "## chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb92fa35-16c9-4d4c-9657-806e83d75061",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./docs/test1/storm.md\", \"r\") as f:\n",
    "    markdown_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35e5a48-7de5-411d-8c49-2707562bc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.experiments.chunk import split_markdown, consolidate_markdown, get_markdown_sections, split_overlap, chunk_chunks\n",
    "from broai.interface import Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92f3b57-ec86-49ad-b037-5455266bcedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown headings: max(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17745/3854812085.py:1: UserWarning: [EXPERIMENT] You're using an experimental function: split_markdown\n",
      "  chunks = split_markdown(markdown_text)\n"
     ]
    }
   ],
   "source": [
    "chunks = split_markdown(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e6aa14-e46b-4074-ac7e-42afe5d8548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef007eb4-3903-4e9c-8b62-88e280b0e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17745/169284658.py:1: UserWarning: [EXPERIMENT] You're using an experimental function: consolidate_markdown\n",
      "  consolidated_chunks = consolidate_markdown(chunks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_chunks = consolidate_markdown(chunks)\n",
    "len(consolidated_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4da9c06-c3ef-4b78-a76e-00f8659b60b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17745/2669657907.py:1: UserWarning: [EXPERIMENT] You're using an experimental function: get_markdown_sections\n",
      "  sections = get_markdown_sections(consolidated_chunks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = get_markdown_sections(consolidated_chunks)\n",
    "len(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b840acec-1a27-4d39-8031-b30e5f24b830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = []\n",
    "source = \".docs/test1/storm.md\"\n",
    "for section, chunk in zip(sections, consolidated_chunks):\n",
    "    contexts.append(Context(context=chunk, metadata={\"section\": section, \"source\": source, \"type\": \"document\"}))\n",
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f86b25e-3be5-4f0f-800b-9661b8053e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17745/246120226.py:1: UserWarning: [EXPERIMENT] You're using an experimental function: split_overlap\n",
      "  new_contexts = split_overlap(contexts)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_contexts = split_overlap(contexts)\n",
    "len(new_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9e613d-dc5b-49a0-86e5-683d73d0e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] | tokens: 35 | chars: 309\n",
      "[1] | tokens: 189 | chars: 1349\n",
      "[2] | tokens: 500 | chars: 4170\n",
      "[3] | tokens: 462 | chars: 3550\n",
      "[4] | tokens: 500 | chars: 2030\n",
      "[5] | tokens: 211 | chars: 1257\n",
      "[6] | tokens: 162 | chars: 1096\n",
      "[7] | tokens: 238 | chars: 1830\n",
      "[8] | tokens: 196 | chars: 2327\n",
      "[9] | tokens: 256 | chars: 1831\n",
      "[10] | tokens: 226 | chars: 1627\n",
      "[11] | tokens: 114 | chars: 759\n",
      "[12] | tokens: 150 | chars: 1002\n",
      "[13] | tokens: 57 | chars: 422\n",
      "[14] | tokens: 166 | chars: 1305\n",
      "[15] | tokens: 105 | chars: 681\n",
      "[16] | tokens: 500 | chars: 1605\n",
      "[17] | tokens: 500 | chars: 1131\n",
      "[18] | tokens: 500 | chars: 868\n",
      "[19] | tokens: 480 | chars: 1431\n",
      "[20] | tokens: 500 | chars: 3018\n",
      "[21] | tokens: 320 | chars: 924\n",
      "[22] | tokens: 196 | chars: 1381\n",
      "[23] | tokens: 500 | chars: 2242\n",
      "[24] | tokens: 500 | chars: 3699\n",
      "[25] | tokens: 260 | chars: 1890\n",
      "[26] | tokens: 441 | chars: 3970\n",
      "[27] | tokens: 88 | chars: 637\n",
      "[28] | tokens: 159 | chars: 1194\n",
      "[29] | tokens: 62 | chars: 413\n",
      "[30] | tokens: 230 | chars: 1487\n",
      "[31] | tokens: 500 | chars: 4696\n",
      "[32] | tokens: 500 | chars: 4683\n",
      "[33] | tokens: 500 | chars: 4837\n",
      "[34] | tokens: 500 | chars: 5164\n",
      "[35] | tokens: 500 | chars: 5514\n",
      "[36] | tokens: 500 | chars: 5094\n",
      "[37] | tokens: 281 | chars: 2320\n",
      "[38] | tokens: 191 | chars: 1398\n",
      "[39] | tokens: 87 | chars: 724\n",
      "[40] | tokens: 500 | chars: 3178\n",
      "[41] | tokens: 500 | chars: 2304\n",
      "[42] | tokens: 500 | chars: 2312\n",
      "[43] | tokens: 299 | chars: 1590\n",
      "[44] | tokens: 124 | chars: 883\n",
      "[45] | tokens: 20 | chars: 232\n",
      "[46] | tokens: 500 | chars: 2032\n",
      "[47] | tokens: 500 | chars: 871\n",
      "[48] | tokens: 500 | chars: 986\n",
      "[49] | tokens: 500 | chars: 755\n",
      "[50] | tokens: 500 | chars: 864\n",
      "[51] | tokens: 500 | chars: 882\n",
      "[52] | tokens: 500 | chars: 1274\n",
      "[53] | tokens: 500 | chars: 1152\n",
      "[54] | tokens: 500 | chars: 1276\n",
      "[55] | tokens: 500 | chars: 2068\n",
      "[56] | tokens: 314 | chars: 1752\n",
      "[57] | tokens: 250 | chars: 1737\n",
      "[58] | tokens: 181 | chars: 1720\n",
      "[59] | tokens: 75 | chars: 613\n",
      "[60] | tokens: 77 | chars: 594\n",
      "[61] | tokens: 99 | chars: 685\n",
      "[62] | tokens: 121 | chars: 896\n",
      "[63] | tokens: 500 | chars: 1743\n",
      "[64] | tokens: 500 | chars: 1425\n",
      "[65] | tokens: 500 | chars: 1261\n",
      "[66] | tokens: 500 | chars: 759\n",
      "[67] | tokens: 500 | chars: 942\n",
      "[68] | tokens: 500 | chars: 1206\n",
      "[69] | tokens: 500 | chars: 826\n",
      "[70] | tokens: 500 | chars: 606\n",
      "[71] | tokens: 412 | chars: 795\n",
      "[72] | tokens: 291 | chars: 1933\n",
      "[73] | tokens: 208 | chars: 1347\n",
      "[74] | tokens: 300 | chars: 1821\n",
      "[75] | tokens: 236 | chars: 1519\n",
      "[76] | tokens: 147 | chars: 952\n",
      "[77] | tokens: 197 | chars: 1297\n",
      "[78] | tokens: 48 | chars: 319\n",
      "[79] | tokens: 93 | chars: 601\n",
      "[80] | tokens: 83 | chars: 505\n",
      "[81] | tokens: 62 | chars: 382\n",
      "[82] | tokens: 90 | chars: 631\n",
      "[83] | tokens: 190 | chars: 1229\n",
      "[84] | tokens: 219 | chars: 1497\n"
     ]
    }
   ],
   "source": [
    "chunk_chunks([c.context for c in new_contexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d28ea5-5a5f-450f-afe8-9edf9ef21aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv-env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
